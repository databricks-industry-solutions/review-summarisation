{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b241969a-5b6a-4bfb-8055-b7bbd1f9c11d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook is available at https://github.com/databricks-industry-solutions/review-summarisation. For more information about this solution accelerator, check out our [website](https://www.databricks.com/solutions/accelerators/large-language-models-retail) and [blog post](https://www.databricks.com/blog/automated-analysis-product-reviews-using-large-language-models-llms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02f973e6-473f-4182-92c9-198d0a837c97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "Given that we have our raw dataframes ready, we will now use this notebook to do some exploration and pre-processing work to prepare our datasets.\n",
    "\n",
    "What we are aiming for here is to bring the data into a format which we can use with an LLM so that it is in a \"ready to be summarised state\". We will try to achieve this by first sampling reviews in a logical way to be able to capture the good and the bad aspects of the books we have got, and then build a text that contains multiple reviews to summarised.\n",
    "\n",
    "Our ultimate goal is to build tis summariser pipeline so we can have a **faster time to action** after we receive reviews, which can be considered as feedback on our products. Therefore, we need to make sure that we capture the most we can from the reviews, which means that we need to pay attention to both good and bad reviews, and prepare our data so that a good amount of both sides make it through. This is quite important from a sampling perspective, because chances are **most products receive more positive reviews than negative** (hinting at an imbalanced dataset), so if we straight up implement a generic sampler, we might loose a good proportion of the negative reviews received by the products.\n",
    "\n",
    "We also need to ensure that the piece of text we are going to send to the LLM doesn't contain too much text, which has to do with the **context lengths** - most of the LLMs' performance begin to degrade with longer context lengths both from an quality perspective (how good is the summary?) and performance perspective (how fast can it run?). So, we need to slice and dice our reviews in a respective way to create sensible batches of reviews.\n",
    "\n",
    "Lets begin!\n",
    "\n",
    "-----\n",
    "\n",
    "**Setup Used:**\n",
    "\n",
    "- Runtime: 13.2 ML\n",
    "- Cluster:\n",
    "  - Machine: 16 CPU + 64 GB RAM (For Driver & Worker)\n",
    "  - 8 Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09bb8742-d258-4090-850a-6b8ad98e1ace",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Initial Setup\n",
    "\n",
    "Here we will begin by setting up the data standards and reading the `raw_book_reviews_df` we created in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85ff749-6793-4eac-96df-82b2edfb5a68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from config import CATALOG_NAME, SCHEMA_NAME, USE_UC\n",
    "\n",
    "# You can skip this line if no-UC\n",
    "if USE_UC:\n",
    "    _ = spark.sql(f\"USE CATALOG {CATALOG_NAME};\")\n",
    "\n",
    "# Sets the standard database to be used in this notebook\n",
    "_ = spark.sql(f\"USE SCHEMA {SCHEMA_NAME};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f42878-03f9-49d5-9839-9cbfbe3ca0ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4x total core count\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 512)\n",
    "\n",
    "# Read the table\n",
    "raw_book_reviews_df = spark.read.table(\"raw_book_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0f5c55d-1445-47b4-9d05-8414fc99f822",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Notebook Parameters\n",
    "\n",
    "In the cell below, you can find some parameters which can help you alter the flow of the notebook from the very top. These can be especially useful if you want to run the notebook without visualisation generation or if you want to apply further sampling to run it faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16209e19-c792-4b40-9cf7-823fc978901b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import TOP_BOOK_COUNT, TOP_BOOK_SAMPLING_FRACTION\n",
    "\n",
    "# Set this to false if you are going to run this as a job\n",
    "run_with_visualisations = False\n",
    "\n",
    "# How many of the top books should we consider ?\n",
    "top_book_count = TOP_BOOK_COUNT\n",
    "\n",
    "# Should we sample these books to work with less data ? (Set this to a fraction like 0.10)\n",
    "top_book_sampling_fraction = TOP_BOOK_SAMPLING_FRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2ee9c9f-7802-4a00-8a01-ca5cb5158108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Helper Display Function\n",
    "We are going to create a new display function to help us with displaying visualisations or skipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fcdf53-7819-4d08-b364-fc819429a75f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def smart_display(df):\n",
    "    # Display only if run with visualisations is true\n",
    "    if run_with_visualisations:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4078eb-4007-4f17-af3a-2074cd994aa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Column Selection\n",
    "The dataframe has many columns, but we don't really need all of them for this specific task. Some might be better suited for other algorithms such as product recommenders, so lets pick and choose what might be useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01c5338-3987-4544-a191-193e71e09629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To see what we have, lets quickly display the data\n",
    "smart_display(raw_book_reviews_df.limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aed8ddb-4770-48d0-90c9-8998cd600e5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are some columns like `tech1` or `fit` which we can remove from the dataframe as well as `also_buy`. Rather than picking the ones that we need to remove, lets focus on the ones we need which might make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67bed3d0-595e-479d-b278-fc73112e0cfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The columns we want\n",
    "focus_columns = [\n",
    "    # Book Attributes\n",
    "    \"asin\",\n",
    "    \"title\",\n",
    "    \"brand AS author\",\n",
    "    \"main_cat AS main_category\",\n",
    "    \n",
    "    # Review Attributes\n",
    "    \"reviewerID AS reviewer_id\",\n",
    "    \"reviewerName AS reviewer_name\",\n",
    "    \"unixReviewTime AS unix_review_time\",\n",
    "    \"overall AS star_rating\",\n",
    "    \"summary AS review_summary\",\n",
    "    \"reviewText AS review_text\",\n",
    "    \"verified\",\n",
    "]\n",
    "\n",
    "# Select the columns wanted\n",
    "book_reviews_df = raw_book_reviews_df.selectExpr(focus_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca57302-33e9-4a18-b2df-5c11e26b2841",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### De-Duplicate\n",
    "\n",
    "We had some issues with duplicating records on the previous notebook, lets see if we can resolve that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b50550b-e56f-4635-99f2-be794aa394ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a count before de-dupe process\n",
    "print(f\"Current row count: {book_reviews_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb7d9ee-77de-4238-9357-14d487183565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Creating a duplicated records dataframe\n",
    "# Reasoning here: There cannot be a reviewer that reviewed the same book more than once at the same time\n",
    "# Therefore, see if there are duplicated reviews\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"asin\", \"reviewer_id\", \"unix_review_time\").count()\n",
    "    .filter(SF.col(\"count\") > 1)\n",
    "    .orderBy(SF.col(\"count\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1b604b3-72b5-4fb7-a13a-927b1ad91561",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like we do have some duplicates, lets see that they look like. There is also a clear pattern here, some specific books have problems with reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a70493-e83b-4220-a0f0-96bfe594e30a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"asin\") == \"B001MQA3DU\")\n",
    "    .filter(SF.col(\"reviewer_id\") == \"A3EBCNHNQIP2Z3\")\n",
    "    .filter(SF.col(\"unix_review_time\") == \"1453248000\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6371f02-dff6-4b16-b9ed-973ef2784c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It seems like specifically this book, The Old Man and the Sea by Ernest Hemingway, which is a great story about an unlucky fisherman, Santiago, had extra problems with duplicates. This is probably a source data problem. Lets check out a different book to see how it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7553719b-31e3-493c-88f9-70a2af2f0156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"asin\") == \"B001MVNGCU\")\n",
    "    .filter(SF.col(\"reviewer_id\") == \"A2HCIFY7GK3VNG\")\n",
    "    .filter(SF.col(\"unix_review_time\") == \"1481068800\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe10e83-7b24-417b-86c4-f3deb40368cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Same problem persists with this book too, lets deduplicate the dataframe by the asin, reviewer_id, and unix_review_time columns and see what we are left with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1079275f-f2d1-4b1f-8434-751829811e77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate by asin, reviewer_id, unix_review_time\n",
    "book_reviews_df = book_reviews_df.dropDuplicates([\"asin\", \"reviewer_id\", \"unix_review_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94dffd34-9f03-459d-83a1-d4b116f88919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check to see if the problem is fixed\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"asin\") == \"B001MQA3DU\")\n",
    "    .filter(SF.col(\"reviewer_id\") == \"A3EBCNHNQIP2Z3\")\n",
    "    .filter(SF.col(\"unix_review_time\") == \"1453248000\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5842838-b613-4fe8-bf9d-b18da64ce6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What about same reviewer reviewing the same item multiple times ? Is this possible or should we allow it ? Lets have a look.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382608b3-917a-417b-ae80-a71abef1211d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check to see if the same reviewer revied the same book multiple times\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"asin\", \"reviewer_id\").count()\n",
    "    .filter(SF.col(\"count\") > 1)\n",
    "    .orderBy(SF.col(\"count\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d235fa-489f-4f93-a5c2-9d73893cc5c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"asin\") == \"1607105551\")\n",
    "    .filter(SF.col(\"reviewer_id\") == \"A1D2C0WDCSHUWZ\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf8eb83-56e2-4945-8b97-d9c6bbb1c3d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"asin\", \"reviewer_id\").count()\n",
    "    .filter(SF.col(\"count\") > 1)\n",
    "    .orderBy(SF.col(\"count\").desc())\n",
    "    .groupby().sum(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f50becb8-3bc1-413b-91a4-25919495f809",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These reviews where the same reviewer reviewed the same product multiple times don't look natural to me. It might be the work of bots or something else. A quick count of these shows that there are 189k of these reviews. We can safely drop them, given that they make a small fraction of our total reviews (we have 51 million reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82288e0b-4ed4-4f88-baea-9ea0e30344a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Current count: {book_reviews_df.count()}\")\n",
    "\n",
    "# Build duplicates \n",
    "de_dupe_df = (\n",
    "    book_reviews_df\n",
    "    .groupBy(\"asin\", \"reviewer_id\")\n",
    "    .count()\n",
    "    .filter(SF.col(\"count\") > 1)\n",
    "    .orderBy(SF.col(\"count\").desc())\n",
    "    .select(\"asin\", \"reviewer_id\")\n",
    ")\n",
    "\n",
    "# Remove with anti join\n",
    "book_reviews_df = book_reviews_df.join(de_dupe_df, on=[\"asin\", \"reviewer_id\"], how=\"leftanti\")\n",
    "\n",
    "print(f\"After count: {book_reviews_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a65292-5a89-4931-8490-f2827efc0c7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Date Transformations\n",
    "Given the UNIX time, we want to extract things like date and time, year month, and year week so we can slice and dice our data properly with different time ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17f8d4f5-63dd-4abe-a344-abf6d372d4ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Create the necessary time frames\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"review_date\", SF.from_unixtime(\"unix_review_time\").cast(\"date\"))\n",
    "    .withColumn(\"week_start\", SF.expr(\"date_sub(review_date, (dayofweek(review_date) - 2) % 7)\"))\n",
    "    .withColumn(\"month_start\", SF.expr(\"trunc(review_date, 'MM')\"))\n",
    "    .withColumn(\"year_start\", SF.expr(\"trunc(review_date, 'YYYY')\"))\n",
    ")\n",
    "\n",
    "smart_display(book_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "384b8d30-bc8e-4eda-b7fd-3f04357ce54e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"year_start\")\n",
    "    .agg(SF.count(\"reviewer_id\"))\n",
    "    .orderBy(SF.col(\"year_start\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f081078-cc4f-460d-927b-b5c8a373ee2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What we can tell from above is that majority of the reviews came after 2012 January. Lets take a closer look there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1cd1dc-caae-4d21-9e05-b5a0ddd82c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .withColumn(\"time_split\", SF.col(\"review_date\") >= \"2012-01-01\")\n",
    "    .groupBy(\"time_split\")\n",
    "    .agg(SF.count(\"reviewer_id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d37a3db-bbe8-4f93-ad4c-7f82460b4fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Verified vs Non-Verified Purchases\n",
    "Lets take a look at verified vs non verified reviewers. The difference is - if a review is verified, it means that the review came from someone who actually purchased the book. Non-verified reviews may include false reviews or inputs from bots who try to spam reviews for multiple reasons. Ideally, we want to keep our data as high quality as possible. So lets see what our distribution looks like over there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968b174f-c9a4-41d2-9bbe-c04be85d65d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"week_start\", \"verified\")\n",
    "    .agg(SF.count(\"reviewer_id\").alias(\"review_count\"))\n",
    "    .orderBy(\"week_start\", \"verified\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e237e12c-b2d3-40ba-a7f8-9e86406db348",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The percentage of the reviews verified increases as time passes, which we can see from the graph above. It shows that in 2013, almost 80% of the received reviews were from verified purchases as opposed to 2005 where only 10% of the reviews were verified.\n",
    "\n",
    "Lets take a look at the percentage of reviews that happened after 2012 Jan which were verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89037732-9154-4b6b-8e7b-d5e881efe7c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"review_date\") >= \"2012-01-01\")\n",
    "    .groupBy(\"verified\")\n",
    "    .agg(SF.count(\"reviewer_id\").alias(\"review_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ac7ac87-4296-4465-947e-370e3e2bcf24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Time & Verification Filters\n",
    "This means that if we take the reviews that happened after 2012, and only include the ones that were verified, we will still have about 32.2 million reviews to work with. This is still a good number, and it can help us with increasing the data quality since we want to make sure that the reviews came from people who actually purchased the products, and using recent data is never a bad idea. Lets apply our filters accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8974a6c4-8019-42db-93ee-49ea219fc842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import pyspark sql functions\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Get count before\n",
    "print(f\"Before count: {book_reviews_df.count()}\")\n",
    "\n",
    "# Apply filters\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"review_date\") >= \"2012-01-01\") \n",
    "    .filter(SF.col(\"verified\") == True)\n",
    ")\n",
    "\n",
    "# Get count after\n",
    "print(f\"After count: {book_reviews_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42cff665-2033-4d8d-bebe-f2d93ea267a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Review Text & Headline Cleaning\n",
    "\n",
    "Some of the reviews and the headlines contain unexpected characters like HTML code or links. We want to get rid of those to make sure only relevant information stays on in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5ef08f-d986-4f8d-ab22-8fc76b02bc35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Taking a look at potential problems\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"review_text\").rlike(\"<a|</a>|href=|hook=\"))\n",
    "    .select(\"review_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c301816f-949a-415d-8da7-d99ffafa4061",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import re\n",
    "\n",
    "# Build Regex for cleaning text\n",
    "remove_regex = re.compile(r\"(&[#0-9]+;|<[^>]+>|\\[\\[[^\\]]+\\]\\]|[\\r\\n]+)\")\n",
    "split_regex = re.compile(r\"([?!.]\\s+)\")\n",
    "\n",
    "# Build UDF for cleaning\n",
    "@SF.udf(\"string\")\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = remove_regex.sub(\" \", text.strip()).strip()\n",
    "    cleaned = \"\"\n",
    "    for fragment in split_regex.split(text):\n",
    "        cleaned += fragment\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Apply UDF\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"review_text\", clean_text(SF.col(\"review_text\")))\n",
    "    .withColumn(\"review_summary\", clean_text(SF.col(\"review_summary\")))\n",
    ")\n",
    "\n",
    "# Making sure we get rid of all\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .filter(~SF.col(\"review_text\").rlike(\"<a|</a>|href=|hook=\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e245790-a2de-4d40-a937-a583437d5afb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Token Count Calculation\n",
    "A term which we are going to see a lot in the upcoming notebooks is Tokens. You can think of tokens like words, but they are a bit different. Large Language Models first transform text in to tokens, which can be interpreted as the way that they read text. These tokens are then encoded in to numbers, which are the mathematical representations of the text pieces we see here. In theory, a large language model actually never sees a word! It only knows of numbers. We can think of these tokenizers as their language translator. \n",
    "\n",
    "Token count becomes an important aspect in this sense, because each LLM has a pre-specified context length, which is bound by token count. For example, if you hear that an LLM has a context length of 2k, that would mean that the longest text it can process can have at most 2048 tokens or so. If you try to feed it more than that, it will error. \n",
    "\n",
    "Therefore, we need to be careful of how many tokens we generate.. Of course, each tokenizer generates a different number of tokens, but we can get to an approximation using the TikToken Library. Lets see how that can be done:\n",
    "\n",
    "A simple math to go from word count to token count is:\n",
    "\n",
    "`n_tokens = n_words * 1.2`\n",
    "\n",
    "This can definitely change, and greatly depends on the tokenizer used. However as a ballpark figure we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ab8431-758e-4d0d-9dc1-0893355d595f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as SF\n",
    "import tiktoken\n",
    "\n",
    "# Build token counter UDF\n",
    "@SF.udf(IntegerType())\n",
    "def calculate_n_tokens(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        num_tokens = len(encoding.encode(string))\n",
    "    except TypeError:\n",
    "        # We implement this part to be able to deal with text that cannot be encoded\n",
    "        num_tokens = -1\n",
    "    return num_tokens\n",
    "\n",
    "# Apply Function\n",
    "book_reviews_df = book_reviews_df.withColumn(\"review_n_tokens\", calculate_n_tokens(\"review_text\"))\n",
    "\n",
    "# Display\n",
    "smart_display(book_reviews_df.select(\"title\", \"review_date\", \"review_text\", \"review_n_tokens\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42d8963-2dda-46fc-8e6d-d8a409e7e5f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check out the negative values (cannot be encoded)\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"review_n_tokens\") == -1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81da20fe-b55e-45a5-9cc7-6a7625f8863a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There wasn't any text which our encoder couldn't deal with, which is great news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de619b0f-12ed-4bdd-bd26-19182fb6f2f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Review Length\n",
    "\n",
    "Now that we have our verified and recent reviews and counted the number of token in each, lets take a look at the review lengths. From what we can tell by some manual inspection, there are some reviews which are quite short (less than 3-4 words long.) Lets check what those look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ceb6483-d7ae-44eb-ab96-7f343341cb0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(book_reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a5b33e-a7d4-4bf2-b409-ea07c6482b2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From what we can tell from this histogram, most of the reviews are less than 200 tokens long. But what does that look like reality ? Also, there is a long tail which we need to get rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9aa056-41bf-4f6d-9f6b-7e0e682bf210",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# What does 500 tokens look like ?\n",
    "import textwrap\n",
    "\n",
    "\n",
    "if run_with_visualisations is True:\n",
    "    print(\n",
    "        textwrap.fill(\n",
    "                book_reviews_df\n",
    "                .filter(SF.col(\"review_n_tokens\").between(490, 510))\n",
    "                .limit(1)\n",
    "                .select(\"review_text\")\n",
    "                .collect()[0][0],\n",
    "            width=80,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22a0916-87b9-4669-b309-6d20859fad94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "500 tokens looks like a lot, we will need to make a decision about what do with these. We will have two options:\n",
    "\n",
    "1. Filter out reviews longer than x amount of tokens\n",
    "2. Cut the reviews short, meaning that take only the first x amount of tokens.\n",
    "\n",
    "Both will come with some information loss. However, 2nd option might be better because it might still tell us something with the risk of context loss (now knowing about the entire text).\n",
    "\n",
    "Before we make a decision, lets see if there is a relationship between token length and star rating. My guess is that a disappointed reviewer might leave a longer review/complaint.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bd8c2b2-a1f1-47f1-8687-c60c7c212e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Star Rating Distribution\n",
    "Each review comes with a star rating which is a scale out of 5, 5 being the highest score (highest satisfaction) and 1 being the lower score (completely dissatisfied). Lets take a look at how this distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2074c3de-a0ab-47a0-b4ac-5ae6dc809966",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .groupBy(\"star_rating\")\n",
    "    .agg(SF.count(\"reviewer_id\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d962bd-052d-4096-9101-7456e60714ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From what we can tell from this pie chart, there is a big imbalance towards 5s & 4s. Combined, they make up for 85% of the reviews. We have to be careful about this imbalance while sampling.\n",
    "\n",
    "Also, there are two reviews with 0 rating ?.. Have to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3441ca-393d-47a0-b6ed-fc2caeb17750",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"star_rating\") == 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06168e4c-551d-4e6d-b531-56b0dac28623",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We'll just assume they are not very happy.. \n",
    "\n",
    "Lets see how the ratings compare against token count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20799eba-2ade-46cc-b96b-babff6b1bf18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Star Rating vs Token Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5cbfd9-5eb5-4518-ade3-8bc4a4d5e390",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"star_rating\") != 0)\n",
    "    .groupBy(\"star_rating\")\n",
    "    .agg(\n",
    "        SF.min(SF.col(\"review_n_tokens\")).alias(\"min_n\"),\n",
    "        SF.median(SF.col(\"review_n_tokens\")).alias(\"median_n\"),\n",
    "        SF.avg(SF.col(\"review_n_tokens\")).alias(\"avg_n\"),\n",
    "        SF.max(SF.col(\"review_n_tokens\")).alias(\"max_n\"),\n",
    "        SF.stddev(SF.col(\"review_n_tokens\")).alias(\"std_n\"),\n",
    "    )\n",
    "    .orderBy(\"star_rating\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d91a1a-487f-4523-9252-6c7df391b974",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It does look like reviews get shorter by their avg and median token count, and standard deviation gets smaller as we move up from 1 to 5 ratings. Our assumption was correct, with the rating 2 being an outlier.\n",
    "\n",
    "We can also tell that the average review is shorter than 70 tokens for all rating groups and the standard deviation is at 136, which means that 70 + 136 = 206 token length will be long enough to capture most of the reviews.\n",
    "\n",
    "Lets see how many reviews have a token count greater than 200 per star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c70a51-dee6-489c-85eb-36d7b441a458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .filter(SF.col(\"star_rating\") != 0)\n",
    "    .withColumn(\"is_long_token\", SF.col(\"review_n_tokens\") > 210)\n",
    "    .groupBy(\"star_rating\", \"is_long_token\")\n",
    "    .agg(SF.count(\"reviewer_id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9019bba-8aa9-47c1-b2c1-8a2e44ebf822",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like more or less 5% of the reviews are too long for each rating. It's good to know that not much of the data will change when we deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7ffc27b-e73c-4572-aee3-96c3a25b2423",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Review Length Shortening\n",
    "\n",
    "Lets go ahead and shorten the reviews rather than filtering them out, and set the max_n_tokens to be 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ad9563b-c1c2-4d63-8ab2-98240cd861fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import tiktoken\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Function to count tokens using tiktoken\n",
    "@SF.udf(\"string\")\n",
    "def truncate_text(text):\n",
    "    encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    tokens = list(encoder.encode(text))\n",
    "    if len(tokens) > 200:\n",
    "        text = encoder.decode(tokens[:200])\n",
    "    return text\n",
    "\n",
    "# Apply UDF\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"review_text\", truncate_text(SF.col(\"review_text\"))) # Truncate\n",
    "    .withColumn(\"review_n_tokens\", calculate_n_tokens(\"review_text\")) # Recount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ea96fb-bb4b-472d-82e3-3566240fed6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "smart_display(book_reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61caf7c0-c32d-4dcc-a385-bda09ea57dde",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now have a much better distribution of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3547b179-8807-4e76-90bd-67a657952311",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Star Rating Classes\n",
    "\n",
    "Given that we are dealing with an imbalanced dataset, we need to deal with the way we process our reviews carefully. If we just sample and take a bunch of reviews, and then try to understand how a product can be improved, we might run into some problems because our sample dataset will be overwhelmed with positive reviews. Therefore, what we can do is create two classes - high_score and low_score. \n",
    "\n",
    "This can assume that a customer who rates a product with 4 or 5 stars is giving it a high score, meaning that they are most probably satisfied, and a customer that gives anything lower than that is giving it a low score, meaning that they are not satisfied with their purchase. \n",
    "\n",
    "Then, we can use the high scores to understand what the customers are happy with, and the low scores to understand what can be improved, or what the customers disliked. \n",
    "\n",
    "Building these classes are quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9d34fa-b7d6-4d3c-b253-6e6d1b111fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Create class\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"star_rating_class\", SF.when(SF.col(\"star_rating\") > 3, \"high\").otherwise(\"low\"))\n",
    ")\n",
    "\n",
    "smart_display(book_reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf8a328-7b15-4939-82f6-2a117a5daa19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Extract Author Name\n",
    "\n",
    "Not sure if it will be needed, but we can extract the Author name. In some cases, author's name is displayed as `Visit Amazon's William Shakespeare Page` probably hinting towards a link. We can easily change this so we extract just the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71775aad-f361-4bc8-95ee-95f84c72b764",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build UDF\n",
    "@SF.udf(\"string\")\n",
    "def extract_author_name(text):\n",
    "    if text.startswith(\"Visit Amazon\"):\n",
    "        text = text.split(\"Amazon's\")[-1]\n",
    "        text = text.rsplit(\"Page\", 1)[0]\n",
    "        text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply UDF\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"author\", extract_author_name(SF.col(\"author\")))\n",
    ")\n",
    "\n",
    "smart_display(book_reviews_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16cd03ad-d3f7-4f8f-8061-d3e71df3bd35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Build Books Metadata\n",
    "\n",
    "Similar to the metadata table we processed in the last notebook, we can go ahead and build a new metadata table, but this time include metrics like total reviews, first review data, total tokens, average score, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e694e4e5-0ee0-4626-8a36-210338440229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Boos metadata\n",
    "books_df = (\n",
    "    book_reviews_df\n",
    "    .groupBy(\"asin\", \"title\", \"author\")\n",
    "    .agg(\n",
    "        SF.min(\"review_date\").alias(\"first_review_date\"),\n",
    "        SF.max(\"review_date\").alias(\"last_review_date\"),\n",
    "        SF.count(\"*\").alias(\"review_count\"),\n",
    "        SF.countDistinct(\"reviewer_id\").alias(\"n_unique_reviewers\"),\n",
    "        SF.round(SF.avg(\"star_rating\"), 2).alias(\"avg_rating\"),\n",
    "        SF.sum(\"review_n_tokens\").alias(\"total_tokens\"),\n",
    "        SF.round(SF.avg(\"review_n_tokens\"), 2).alias(\"avg_tokens\")\n",
    "    )\n",
    "    .orderBy(SF.col(\"review_count\").desc())\n",
    ")\n",
    "\n",
    "# Display\n",
    "smart_display(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "876e0ee4-58c2-4629-b15a-ac1d65bfeb21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets check out how many reviews we can cover if we take top books (ordered by number of reviews received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37af1e2a-46bd-47a4-a650-022f74a5de62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using the top book count defined at the top of the notebook\n",
    "n_books = top_book_count\n",
    "\n",
    "smart_display(\n",
    "    books_df\n",
    "    .orderBy(SF.col(\"review_count\").desc())\n",
    "    .limit(n_books)\n",
    "    .groupBy()\n",
    "    .sum(\"review_count\").alias(\"total_review_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77af87cc-f60a-473b-8094-1f01113eef1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Book Sampling\n",
    "If we take the top 1000 popular books we end up with a sample which contains 3.8M reviews. In a real world scenario, our goal would be to process all the reviews, however for showcasing purposes understanding how we can deal with 3.8M reviews should be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d583c326-7a2c-472f-8cab-d73290a82572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the top books\n",
    "n_books = top_book_count\n",
    "sampling_fraction = top_book_sampling_fraction\n",
    "\n",
    "# Limit\n",
    "books_df = (\n",
    "    books_df\n",
    "    .orderBy(SF.col(\"review_count\").desc())\n",
    "    .limit(n_books)\n",
    ")\n",
    "\n",
    "# Sample if specified\n",
    "if sampling_fraction and 0.0 < sampling_fraction < 1.0:\n",
    "    print(\"Further sampling\")\n",
    "    books_df = books_df.sample(False, fraction=sampling_fraction, seed=42)\n",
    "\n",
    "smart_display(books_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8e7091-1eb0-4251-8455-0c67a1babb31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reviews Sampling\n",
    "\n",
    "Same as we did for books, we can go ahead and sample our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33507d2a-e921-497e-a2d3-900ad1efc9e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join with the sampled books df to apply sample in reviews\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .join(books_df.select(\"asin\"), how=\"inner\", on=\"asin\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d94c0f-39f9-4224-abb4-aa0f1866429e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Build Review Batches\n",
    "\n",
    "We have our reviews dataframe ready. Now, its time to prepare these reviews in batches so we can start extracting summaries.\n",
    "\n",
    "We will group our reviews by `product`, `rating class`, and `week start`, however we will then have some weeks where a book received many reviews which may increase token count by a lot and make it harder to process using our LLM model, therefore we want to create batches as well.\n",
    "\n",
    "Aiming to create batches which have about 800-1000 tokens each per product, ordered by date makes sense here. This won't be an exact science though.\n",
    "\n",
    "So, for example, if a certain product received 5 reviews which in total makes up about 800 tokens, we want to call that batch 1, and the next group of reviews will then start falling into batch 2 until they make up 800 tokens as well.\n",
    "\n",
    "We need to batch the reviews like this, because our ultimate goal is to feed the LLM model about 800-1000 tokens of reviews or less at a time due to context length considerations. To recap, context length has to do with how much information is passed to the LLM at a given time. Even though some models have larger context lengths, there has been studies which show that model performance and accuracy degrades as context length increases.\n",
    "\n",
    "If we were to think about this from a real life like perspective, imagine that rather than the LLM, we are tasked with summarising a given set of reviews in to bullet points. If we maybe read 2 or 3 reviews, and try to create bullet points out of that, we can potentially do it without having to go back to text our without forgetting about what we read. However, if we were to read 10 reviews at a time, we might have a harder time remembering everything or summarising the information we just read into bullet points. \n",
    "\n",
    "The same can be said for LLMs: processing gets harder, and quality drops.. Therefore, creating mini batches of reviews like this is a good idea.\n",
    "\n",
    "One thing to clarify: we are not going to cut a certain review in half just to fill up the batch limit. For example, if the first 4 reviews add up to 800 tokens, and if the 5th review has 150 tokens (which would push the batch over the limit), we are simply going to take the first 4 reviews, call it a batch, and then start the new batch with the 5th review.\n",
    "\n",
    "We are also going to create separate batches for high score and low score reviews (based on the classes we have created before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f882c1a-602f-45b6-ae54-29886e24e8e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Set max n_tokens per batch\n",
    "max_n_tokens = 800\n",
    "\n",
    "# Define the window partitioned by product id (asin) and rating class, order it by date and reviewer_id\n",
    "batch_window = (\n",
    "    Window\n",
    "    .partitionBy(\"asin\", \"week_start\", \"star_rating_class\")\n",
    "    .orderBy(\"review_date\", \"reviewer_id\")\n",
    ")\n",
    "\n",
    "# Calculate the cumulative sum with respect to the window\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"cumulative_n_tokens\", SF.sum(\"review_n_tokens\").over(batch_window))\n",
    "    .orderBy(\"asin\", \"star_rating_class\", \"review_date\", \"reviewer_id\")\n",
    ")\n",
    "\n",
    "# Take a look at the data\n",
    "smart_display(\n",
    "    book_reviews_df.select(\n",
    "        \"asin\",\n",
    "        \"title\",\n",
    "        \"star_rating_class\",\n",
    "        \"review_date\",\n",
    "        \"review_n_tokens\",\n",
    "        \"cumulative_n_tokens\",\n",
    "    ).limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca094c1-9541-43e0-9a24-debb9aa35d02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We just calculated the cumulative number of tokens per book and per star rating class. But how is this going to help us ? How can we get to batches from here ?\n",
    "\n",
    "The answer is quite simple - if we divide the cumulative_n_tokens column by our max token limit, and then round that number up (ceil it), that should do the trick.\n",
    "\n",
    "For example, theoretically speaking, in the above example the cutoff point for the first batch should be at the 9th row, where the 9th row marks the end of the batch 1 (since it will have 800 tokens), and then the 10th row should be the first example in batch 2. \n",
    "\n",
    "If we divide the cumulative_n_tokens by 800, for the rows 9 and above, that is going to produce a results between 0 and 1. And then rounding that result up will mark those rows as batch 1. \n",
    "\n",
    "This won't always yield the same result though, **some of the batches might have tokens sizes going up to 950**, but thats OK.\n",
    "\n",
    "Lets implement this and have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f9e21a-ec2f-45d7-a2cb-2687fd9f0738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Create batch column\n",
    "book_reviews_df = (\n",
    "    book_reviews_df\n",
    "    .withColumn(\"batch_id\", SF.ceil(SF.col(\"cumulative_n_tokens\") / max_n_tokens))\n",
    ")\n",
    "\n",
    "smart_display(\n",
    "    book_reviews_df\n",
    "    .select(\"asin\", \"title\", \"star_rating_class\", \"review_date\", \"week_start\", \"review_n_tokens\", \"cumulative_n_tokens\", \"batch_id\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f135357-a267-42f0-bcae-d4e7a08b5585",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Aggregate Reviews\n",
    "\n",
    "Now that we have created our mini batches to be processed, what we can do is to aggregate our reviews with respect to product ids, star rating class and batch id. What does aggregating reviews mean though ? It means that we want make one big review by combining all the reviews into a single text piece by concatenation. So, it will be like review_1 + review_2 + .. + review_n in a batch as a single string. This is what we will pass to the LLM to process and extract insights from.\n",
    "\n",
    "We have grouped this by star rating class as well, since there is an imbalance between ratings and since we want to make sure we understand the points which the customers are not happy about. So, after we have done our aggregation, we are going to ask our LLM to extract the points which the customers are happy about form high scored reviews, and extract the points which customers are dissatisfied about from the low scored reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee480512-6393-4698-a815-6bf33d5c2c54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Create batched book reviews\n",
    "batched_book_reviews_df = (\n",
    "    book_reviews_df.groupBy(\n",
    "        \"asin\",\n",
    "        \"title\",\n",
    "        \"author\",\n",
    "        \"star_rating_class\",\n",
    "        \"week_start\",\n",
    "        \"batch_id\",\n",
    "    )\n",
    "    .agg(\n",
    "        SF.sum(\"review_n_tokens\").alias(\"n_tokens\"),\n",
    "        SF.count(\"reviewer_id\").alias(\"n_reviews\"),\n",
    "        SF.round(SF.avg(\"star_rating\"), 2).alias(\"avg_star_rating\"),\n",
    "        SF.min(\"review_date\").alias(\"first_review_date\"),\n",
    "        SF.max(\"review_date\").alias(\"last_review_date\"),\n",
    "        SF.collect_list(\"review_text\").alias(\"review_array\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"concat_review_text\", \n",
    "        SF.concat_ws(\" \", SF.col(\"review_array\"))\n",
    "    )\n",
    "    .drop(\"review_array\")\n",
    "    .orderBy(\n",
    "        \"asin\",\n",
    "        \"star_rating_class\",\n",
    "        \"week_start\",\n",
    "        \"batch_id\",\n",
    "    )\n",
    ")\n",
    "\n",
    "smart_display(batched_book_reviews_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdb3c49-2f2a-4619-99aa-1a5d78759c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Get some summary statistics on the data\n",
    "display(\n",
    "    batched_book_reviews_df\n",
    "    .groupBy()\n",
    "    .agg(\n",
    "        SF.countDistinct(\"asin\").alias(\"n_books\"),\n",
    "        SF.count(\"*\").alias(\"n_batches\"),\n",
    "        SF.sum(\"n_reviews\").alias(\"n_reviews\"),\n",
    "        SF.sum(\"n_tokens\").alias(\"total_tokens\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e7c9f1-ad3a-45c2-9d90-019a57fc0a26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Save the Data\n",
    "We have\n",
    "* Cleaned the book reviews\n",
    "* Analysed our data's distribution across multiple dimensions\n",
    "* Created a new books metadata table\n",
    "* Sampled and dealt with imbalances\n",
    "* Aggregated and batched our reviews\n",
    "\n",
    "At the end, we ended up with:\n",
    "\n",
    "| n_books   | n_batches   | n_reviews   | total_tokens   |\n",
    "|:----------|:------------|:------------|:---------------|\n",
    "| 1000      | 456.1K      | 3.8M        | 156M           |\n",
    "\n",
    "_This was achieved with setting top book count to 1000, and without any further sampling_\n",
    "\n",
    "Lets go ahead and save our new dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f6f08c-a573-47bc-8ed9-d150b676211c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Book Reviews\n",
    "(\n",
    "    book_reviews_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"cleaned_book_reviews\")\n",
    ")\n",
    "\n",
    "# Books\n",
    "(\n",
    "    books_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"cleaned_books\")\n",
    ")\n",
    "\n",
    "# Batched Book Reviews\n",
    "(\n",
    "    batched_book_reviews_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"batched_book_reviews\")\n",
    ")\n",
    "\n",
    "# Optimize All\n",
    "_ = spark.sql(\"OPTIMIZE cleaned_book_reviews;\")\n",
    "_ = spark.sql(\"OPTIMIZE cleaned_books;\")\n",
    "_ = spark.sql(\"OPTIMIZE batched_book_reviews;\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-explore-prep",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
