{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "795d8769-5cb4-4fec-8c7c-0685af033c13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook is available at https://github.com/databricks-industry-solutions/review-summarisation. For more information about this solution accelerator, check out our [website](https://www.databricks.com/solutions/accelerators/large-language-models-retail) and [blog post](https://www.databricks.com/blog/automated-analysis-product-reviews-using-large-language-models-llms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e9f5d1-5435-4bae-bbba-fdd445505d74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Summarisation\n",
    "\n",
    "Our instructions are ready and the reviews are waiting to be summarised! We can now take the necessary steps to begin our inference (summirisation task).\n",
    "\n",
    "Before we do so, it might help to do a couple of things.. We want to optimise the speed of inference as much as possible (without trading off quality) and we also want to distribute our inference so we can scale properly. \n",
    "\n",
    "In this notebook, we will cover the optimisations that can be done pre-summarisation, and how we can parallelize the work.\n",
    "\n",
    "---\n",
    "\n",
    "**Setup Used:**\n",
    "\n",
    "- Runtime: 13.2 ML + GPU\n",
    "- Cluster:\n",
    "  - Machine: GPU with > 20GB (For Driver & Worker) \n",
    "  - 3+ Workers\n",
    "  - Required GPUs: Nvidia A100 or A10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "850228a5-eff1-42e5-87b6-c3c1b9e58a62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Library Installation\n",
    "\n",
    "We can start by installing the libraries we are going to need for this work. As always, you can choose to specify these using the cluster's configuration page so that the cluster can auto spawn with these libraries installed. Another benefit - the libraries stay there even if you detach from the notebook (which won't be the case here..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0aeaad-71d2-431b-b21a-9985519fdd5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "%pip install -qq flash-attn\n",
    "%pip install -qq xformers\n",
    "%pip install -qq torch==2.0.1\n",
    "%pip install -qq ctranslate2==3.17\n",
    "%pip install -qq triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python\n",
    "\n",
    "# Restart Python Kernel\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64cf736-ce7b-440c-bb84-f4dcd01d937d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data Defaults\n",
    "Specifying our data defaults for catalog and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0131ee89-75c2-4dbe-90e4-ddc902a931ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from config import CATALOG_NAME, SCHEMA_NAME, USE_UC\n",
    "\n",
    "# If UC is enabled\n",
    "if USE_UC:\n",
    "    _ = spark.sql(f\"USE CATALOG {CATALOG_NAME};\")\n",
    "\n",
    "# Sets the standard database to be used in this notebook\n",
    "_ = spark.sql(f\"USE SCHEMA {SCHEMA_NAME};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3560f6ea-f8c9-4df1-930e-d9320270ae7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Paths\n",
    "Specifying the paths we are going to use in this notebook.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c49f90e-4234-49e0-8b57-118ad9c0495a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the OS system to declare a ENV variable\n",
    "from config import MAIN_STORAGE_PATH\n",
    "import os\n",
    "\n",
    "# Setting up the storage path (please edit this if you would like to store the data somewhere else)\n",
    "main_storage_path = f\"{MAIN_STORAGE_PATH}/model_store\"\n",
    "\n",
    "# Declaring as an Environment Variable \n",
    "os.environ[\"MAIN_STORAGE_PATH\"] = main_storage_path\n",
    "\n",
    "# Set local model paths\n",
    "local_model_path = f\"{main_storage_path}/mpt-7b-instruct\"\n",
    "local_tokenizer_path = f\"{main_storage_path}/mpt-7b-tokenizer\"\n",
    "local_model_optimised_path = f\"{main_storage_path}/mpt-7b-ct2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c525982-828b-415e-bdb1-9c9fe4215509",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GPU Determination\n",
    "\n",
    "We want to determine whether we are using A10s or A100s. Depending on this, we are going to adjust our batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90396913-77cb-4b52-ab4a-bb5e906f81d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get available GPU memory\n",
    "total_mem = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2  # in MB\n",
    "\n",
    "# Decide depending on memory\n",
    "gpu_type = \"small\" if total_mem < 70000 else \"large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7552a47-aea6-4844-a50e-69d7f505e9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Retrieval\n",
    "\n",
    "We created the batched instructions dataset in the last notebook, which was produced after our prompt engineering tests. This dataset includes a `model_instruction` column, which has the text we are going to feed to the LLM with the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60546db2-a40b-4825-9b46-881d24c4f63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the instructions dataframe\n",
    "instructions_df = spark.read.table(\"batched_instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec937bbb-3984-4f9b-88c0-c71999758504",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inferecene Optimisations\n",
    "\n",
    "Lets see if we can optimise the speed of our inference.. There is a library called `CTranslate2` which can take existing transformer like models, and optimise them for inference. This can help us greatly, and reduce the resources we may need to use.\n",
    "\n",
    "The library works by converting an existing transformer into a generator. Which essentially has the same properties, but with some added options. \n",
    "\n",
    "This library offers quantisation as well.. Quantisation helps with making the model run with a smaller footprint on the GPU. However, it comes with a trade-off - the answer quality begins to drop as you quantise further. \n",
    "\n",
    "But, for some cases it might still make sense. If you would like use a more performant quantisation, you can definitely lower it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c08250-2ab8-4651-8d44-22692f78081c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from ctranslate2.converters import TransformersConverter\n",
    "\n",
    "# Initiate the converter\n",
    "if os.path.isdir(local_model_optimised_path):\n",
    "    print(\"Optimised model exists\")\n",
    "else:\n",
    "    mpt_7b_converter = TransformersConverter(\n",
    "        model_name_or_path=local_model_path,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Request conversion\n",
    "    mpt_7b_converter.convert(\n",
    "        output_dir=local_model_optimised_path,\n",
    "        quantization=\"bfloat16\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3f31c9b-b4bb-438d-9d0c-e9a6d04dcdb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Converted Model & Tokenizer\n",
    "\n",
    "Our model has been converted and is now ready to do be tested for inference. Let's load it up with the tokenizer, and see what we can do.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3906b8c-1fdf-411e-909f-79973cde0b00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from transformers import AutoTokenizer\n",
    "import ctranslate2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the paths\n",
    "local_tokenizer_path = f\"{main_storage_path}/mpt-7b-tokenizer\"\n",
    "local_model_optimised_path = f\"{main_storage_path}/mpt-7b-ct2\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the model\n",
    "mpt_optimised_model = ctranslate2.Generator(\n",
    "    model_path=local_model_optimised_path,\n",
    "    device=\"cuda\",\n",
    "    device_index=0,\n",
    "    compute_type=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e402930a-c7f4-4d40-b383-192095c69c19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Test Flow Build\n",
    "\n",
    "We can build a test flow to see how the model does, and experiment with some parameters. We especially want to focus on the batch size parameter here to find it's sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d26c000b-dca7-4c7c-970b-8b477158ed7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_inference(requests, batch_size):\n",
    "    \n",
    "    # Create a return dict\n",
    "    return_dict = {}\n",
    "\n",
    "    # Time\n",
    "    encoding_start = time.time()\n",
    "\n",
    "    # Encode requests with tokenizer\n",
    "    batch_tokens = [tokenizer.encode(x) for x in requests]\n",
    "    batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in batch_tokens]\n",
    "\n",
    "    # Time\n",
    "    return_dict[\"encoding_time\"] = round(time.time() - encoding_start, 4)\n",
    "    generation_start = time.time()\n",
    "\n",
    "    # Generate results with the model\n",
    "    batch_results = mpt_optimised_model.generate_batch(\n",
    "        batch_tokens,\n",
    "        max_batch_size=batch_size,\n",
    "        max_length=150,\n",
    "        include_prompt_in_result=False,\n",
    "        sampling_temperature=0.1,\n",
    "    )\n",
    "    \n",
    "    # Time\n",
    "    return_dict[\"generation_time\"] = round(time.time() - generation_start, 4)\n",
    "    decoding_start = time.time()\n",
    "\n",
    "    # Decode results with the tokenizer\n",
    "    decoded_results = [tokenizer.decode(x.sequences_ids[0]) for x in batch_results]\n",
    "\n",
    "    # Time\n",
    "    return_dict[\"decoding_time\"] = round(time.time() - decoding_start, 4)\n",
    "    return_dict[\"total_time\"] = round(time.time() - encoding_start, 4)\n",
    "\n",
    "    # Prepare and Return\n",
    "    return_dict[\"results\"] = decoded_results\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8ab52f1-ec42-4c33-af81-cf4e3fdee134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Retrieving few examples from our dataset here so we can do some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbcedbd-ea38-4207-a3e4-37da0377f6c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Random sample examples\n",
    "examples = (\n",
    "    instructions_df\n",
    "    .sample(False, 0.01, seed=42)\n",
    "    .select(\"model_instruction\")\n",
    "    .limit(100)\n",
    "    .collect()\n",
    ")\n",
    "examples = [x[0] for x in examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fd27249-11be-4fa1-9b56-6815609db31d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The code below can help us with identifying the optimal spot for the batch size parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8133e8c2-9f7d-4e9c-8eb1-09d94dcfb1f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Batch sizes to be tested\n",
    "batch_size_test = [1, 5, 10]\n",
    "\n",
    "if gpu_type == \"large\":\n",
    "    batch_size_test += [15, 20, 25, 30, 35]\n",
    "\n",
    "# Speed Test\n",
    "for batch_test in batch_size_test:\n",
    "    try:\n",
    "        print(\"-\"*15)\n",
    "        print(\"Batch Size\", batch_test)\n",
    "        _result = run_inference(examples, batch_size=batch_test)\n",
    "        print(_result[\"total_time\"])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63bbe40b-a48b-4947-8752-8079a3a60409",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like **20** is the number we are looking for if we have `A100`. \n",
    "**10** might be better suited for `A10`.\n",
    "Let see how the results look like when we use this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900adf29-1eb7-47bb-b011-cde05d5a8bae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine the ideal batch size\n",
    "ideal_batch_size = 20 if gpu_type == \"large\" else 10\n",
    "\n",
    "results = run_inference(examples, batch_size=ideal_batch_size)\n",
    "\n",
    "for key in results.keys():\n",
    "    if \"time\" in key:\n",
    "        print(f\"{key}: {results[key]}\")\n",
    "\n",
    "for _request, _response in zip(examples, results[\"results\"]):\n",
    "    print(\"-\" * 15)\n",
    "    print(_request)\n",
    "    print()\n",
    "    print(_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb63091d-2f91-4a2c-bea0-2357c2bb85d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Distributed Inference\n",
    "\n",
    "Now that our test flow works and we have a batch size we can use, lets build a similar flow, but this time to be distributed across a cluster.\n",
    "\n",
    "In the code below, what we will try to do is to create the entire inference needed to process the instructions, and pack it all up in a `Pandas UDF`. By this way, we will get to execute the same flow in each worker.\n",
    "\n",
    "When this code us run as a UDF on our dataframe, each worker is going to get a copy of it alongside with a piece of data, and an identical copy of the model we have created with `CTranslate` will be loaded in each worker to process the data.\n",
    "\n",
    "This can be done thanks to the nature of the `Pandas UDF`, which differs from the regular `PySpark UDF`. Pandas UDFs get to process the data in chunks as Pandas Series, where as Spark UDFs process the data row by row.. So one gets called per row, and the other gets called per data chunk.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750f736d-e02b-46a1-9eed-e616d1d08dbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# External Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "\n",
    "# Build Inference Function\n",
    "@SF.pandas_udf(\"string\", SF.PandasUDFType.SCALAR_ITER)\n",
    "def run_distributed_inference(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "\n",
    "    # External Imports\n",
    "    from transformers import AutoTokenizer\n",
    "    import ctranslate2\n",
    "    import torch\n",
    "\n",
    "    # Define the paths\n",
    "    local_tokenizer_path = f\"{main_storage_path}/mpt-7b-tokenizer\"\n",
    "    local_model_optimised_path = f\"{main_storage_path}/mpt-7b-ct2\"\n",
    "\n",
    "    # Understand GPU size\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2  # in MB\n",
    "\n",
    "    # Decide depending on memory\n",
    "    gpu_type = \"small\" if total_mem < 70000 else \"large\"\n",
    "\n",
    "    # Params\n",
    "    temperature = 0.1\n",
    "    max_new_tokens = 150\n",
    "    batch_size = 20 if gpu_type == \"large\" else 10\n",
    "    repetition_penalty = 1.05\n",
    "    top_k = 50\n",
    "    top_p = 0.9\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path, padding_side=\"left\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load the model\n",
    "    mpt_optimised_model = ctranslate2.Generator(\n",
    "        model_path=local_model_optimised_path,\n",
    "        device=\"cuda\",\n",
    "        device_index=0,\n",
    "        compute_type=\"bfloat16\"\n",
    "    )\n",
    "\n",
    "    for requests in iterator:\n",
    "\n",
    "        # Encode requests with tokenizer\n",
    "        batch_tokens = [tokenizer.encode(x) for x in requests.to_list()]\n",
    "        batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in batch_tokens]\n",
    "\n",
    "        # Batch results\n",
    "        batch_results = mpt_optimised_model.generate_batch(\n",
    "            batch_tokens,\n",
    "            max_batch_size=batch_size,\n",
    "            max_length=max_new_tokens,\n",
    "            include_prompt_in_result=False,\n",
    "            sampling_temperature=temperature,\n",
    "            sampling_topk=top_k,\n",
    "            sampling_topp=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "\n",
    "        # Batch decode\n",
    "        decoded_results = [tokenizer.decode(x.sequences_ids[0]) for x in batch_results]\n",
    "\n",
    "        yield pd.Series(decoded_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db82320-1d0c-45da-9e95-79da871901f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply Inference on Dataframe\n",
    "\n",
    "Our inference function is ready to go, we can now map it to apply it to our dataframe.\n",
    "\n",
    "Over here, we want to set the number of repartition to the number of worker nodes we have in our cluster. For example, if we have 1 driver node and 3 worker nodes (as specified in the setup), then we want to set the repartition number to 3. We have some code which can automatically set the number of repartition to the number of worker nodes we have. However, this can be overridden if needed. (For example, for multiple gpu per worker nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc66922-5693-4f3d-a094-28954f308875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Auto get number of workers\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Subtract 1 to exclude the driver\n",
    "num_workers = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1  \n",
    "\n",
    "# Set the batch size for the Pandas UDF\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", num_workers*1000)\n",
    "\n",
    "# Repartition\n",
    "instructions_df = instructions_df.repartition(num_workers)\n",
    "\n",
    "# Run Inference\n",
    "instructions_df = (\n",
    "    instructions_df\n",
    "    .withColumn(\"llm_summary\", run_distributed_inference(SF.col(\"model_instruction\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbc901a5-83dc-45da-84d2-955c33c4d784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save the Dataframe\n",
    "\n",
    "As for the final step of our notebook, we can go ahead and save our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ea8898-0943-4897-b1a8-b68c236f8353",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "(\n",
    "    instructions_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"book_reviews_summarised\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-summarisation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
