{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf72326-fa62-413a-8bac-430579ed09aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook is available at https://github.com/databricks-industry-solutions/review-summarisation. For more information about this solution accelerator, check out our [website](https://www.databricks.com/solutions/accelerators/large-language-models-retail) and [blog post](https://www.databricks.com/blog/automated-analysis-product-reviews-using-large-language-models-llms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b89a2dab-ac94-41ba-964c-146d1b4695c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Presentation\n",
    "\n",
    "We have used our model and our summarisation task is complete. As for the short and last step, the only thing that is left to be done is to turn our dataframe into an easily presentable format.\n",
    "\n",
    "What we want to aim for is a dataframe that has a row per book per week. In each row, we want to have some metadata information such as book name, author, etc.. as well as avg. rating for the week, positive summaries and negative summaries. This can greatly help if we need to build a dashboard.\n",
    "\n",
    "----\n",
    "\n",
    "**Setup Used:**\n",
    "\n",
    "- Runtime: 13.2 ML\n",
    "- Cluster:\n",
    "  - Machine: 16 CPU + 64 GB RAM (For Driver & Worker)\n",
    "  - 8 Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccbdf3b1-e78f-464e-9cef-66953bcc3fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Data Defaults\n",
    "Specify catalog and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23cbb13e-6d07-4b3c-bcee-a32f584092e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from config import CATALOG_NAME, SCHEMA_NAME, USE_UC\n",
    "\n",
    "# If UC is enabled\n",
    "if USE_UC:\n",
    "    _ = spark.sql(f\"USE CATALOG {CATALOG_NAME};\")\n",
    "\n",
    "# Sets the standard database to be used in this notebook\n",
    "_ = spark.sql(f\"USE SCHEMA {SCHEMA_NAME};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fff3ad8-8390-4b82-b7c6-da2228f8e350",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Read Data\n",
    "Read the summarised and condensed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba8732d7-5148-470b-b556-2e84c613487a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4x total core count\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 512)\n",
    "\n",
    "# Read the table\n",
    "reviews_df = spark.read.table(\"book_reviews_condensed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c8bf8bd-1be3-481c-bede-8f1bd88f5289",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build Meta DF\n",
    "This dataframe will have the per week per book information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b481cfc-b226-49cd-a688-4c9b642f33c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build meta reviews df\n",
    "meta_reviews_df = (\n",
    "    reviews_df\n",
    "    .withColumn(\n",
    "        \"weighted_star_rating\", \n",
    "        SF.col(\"n_reviews\") * SF.col(\"avg_star_rating\")\n",
    "    )\n",
    "    .groupBy(\"asin\", \"title\", \"author\", \"week_start\")\n",
    "    .agg(\n",
    "        SF.sum(\"n_reviews\").alias(\"n_reviews\"),\n",
    "        SF.sum(\"n_review_tokens\").alias(\"n_review_tokens\"),\n",
    "        SF.sum(\"weighted_star_rating\").alias(\"weighted_star_rating\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"avg_star_rating\", \n",
    "        SF.round(SF.col(\"weighted_star_rating\") / SF.col(\"n_reviews\"), 2),\n",
    "    )\n",
    "    .drop(\"weighted_star_rating\")\n",
    "    .orderBy(\"asin\", \"title\", \"author\", \"week_start\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7301df35-a640-4191-bd84-146148dd923a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build Summary Reviews\n",
    "This dataframe will have positive and negative reviews placed in the same row rather than having separate rows for each. We will use a pivot function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66354ed2-0c74-4343-9666-d62d5c7bd311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build meta reviews df\n",
    "summary_reviews_df = (\n",
    "    reviews_df.groupBy(\"asin\", \"title\", \"author\", \"week_start\")\n",
    "    .pivot(\"star_rating_class\")\n",
    "    .agg(SF.first(\"final_review_summary\"))\n",
    "    .withColumnRenamed(\"high\", \"positive_reviews_summary\")\n",
    "    .withColumnRenamed(\"low\", \"negative_reviews_summary\")\n",
    "    .orderBy(\"asin\", \"title\", \"author\", \"week_start\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ea7760-cf94-4b77-a852-8d8cfe38a415",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Â Join Dataframes\n",
    "Join the two dataframes we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "746ba3f0-9b0b-4524-b25a-4ca7aeade500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_df = meta_reviews_df.join(\n",
    "    summary_reviews_df, \n",
    "    how=\"inner\", \n",
    "    on=[\"asin\", \"title\", \"author\", \"week_start\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e098b7d-bdb5-4096-bd12-4399cd5fa289",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parse as HTML \n",
    "Parse the summary cells as HTML columns so we can display nicely on our dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cee4cb8-364d-4db2-aecc-27acfa50e2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "import html\n",
    "\n",
    "# Build a UDF to convert to HTML\n",
    "@SF.udf(\"string\")\n",
    "def convert_to_html(text):\n",
    "    html_content = \"\"\n",
    "    try:\n",
    "        # Escape any existing HTML characters\n",
    "        escaped_string = html.escape(text)\n",
    "        # Replace newline characters with HTML line breaks\n",
    "        html_content = escaped_string.replace(\"\\n\", \"<br>\")\n",
    "    except:\n",
    "        pass\n",
    "    return html_content\n",
    "\n",
    "# Apply \n",
    "summary_df = (\n",
    "    summary_df\n",
    "    .withColumn(\"positive_reviews_summary\", convert_to_html(\"positive_reviews_summary\"))\n",
    "    .withColumn(\"negative_reviews_summary\", convert_to_html(\"negative_reviews_summary\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "623cef81-df19-4fc8-bbe7-920b524550fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Build Display ID\n",
    "We might have some occurrences where a book might have the same name with another one, therefore we want to create a unique display ID thats made from book's name, author's name, and the ID of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d4efdc8-5a11-4f2e-960b-4931d129c063",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "# Build UDF \n",
    "@SF.udf(\"string\")\n",
    "def build_display_id(title, author, asin):\n",
    "    display_id = f\"{title} by {author} ({asin})\"\n",
    "    return display_id\n",
    "\n",
    "# Apply\n",
    "summary_df = summary_df.withColumn(\n",
    "    \"display_id\", build_display_id(SF.col(\"title\"), SF.col(\"author\"), SF.col(\"asin\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef05a28-dd6a-4771-a412-066ae063fa6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save Finalised Dataframe\n",
    "\n",
    "And our final product is ready.. we can go ahead and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36c248e-b727-474c-97e4-b02d9daf1201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    summary_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(\"book_reviews_finalised\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "464b5e72-8846-47d5-af87-d6382581932d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(summary_df.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06-presentation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
